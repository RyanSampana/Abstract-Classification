{
    "collab_server" : "",
    "contents" : "#' Calculates likelihoods, priors, and evidence for classifying text data by\n#' Category\n#'\n#' Calculates likelihoods, priors, and evidence for classifying text data by\n#' Category for a particular word P(Category|word). The function determines:\n#' P(word|Category) as the frequency of the word within each Category divided\n#' by the total number of words within each Category; P(word) is the frequency\n#' of the word among all Categories divided by the number of words among all\n#' Categories; P(Category) is calculated as the total number of words within\n#' each Category divided by the total number of words in all Categories.\n#' (There is an option to adjust the influence of the prior.)\n#'\n#' @param trainIn A list of character vectors. (Output from cleanData())\n#' @param trainOut A character vector denoting the Category to which each\n#' element of the trainIn list belongs.\n#' @param alpha The smoothing term for words whose frequencies are 0.\n#' @param priors A vector of prior probabilities for each category. The order\n#' matters and is as follows c(Math, Comp, Phys, Stat, Misc). If this is\n#' left NULL, the default method to calculate priors is used.\n#' (Including a prior for the Misc Category is optional.)\n#' @param diffuse A number between 0 and 1 that scales the influence of\n#' the prior, P(Category). (If priors are self-specified, this does nothing.)\n#' P(Category') = P(Category)*(diffuse) + (1/4)*(1-diffuse).\n#' This is a simple scaling system where, when diffuse=0, a uniform\n#' prior is used (1/4 for each Category), when 1, the whole prior is used.\n#'\n#' @return A list with two entries. The first is a dataframe with six columns.\n#' The first column identifies is a vector of words and the last column is\n#' the evidence for each word. The middle five columns are the likelihoods\n#' for each word for the Categories Math, Comp, Phys, Stat, and Misc, in that\n#' order EDIT: (Misc is never used, so it's actually removed before output).\n#' @export\n#'\n#' @examples\n#' #postProbs <- bayesProb(cleanTrainIn,cleanTrainOut,diffusePrior=0.1)\nbayesProb <-\n  function(trainIn,\n           trainOut,\n           alpha = 1,\n           diffuse = 1,\n           priors = NULL\n) {\n    abstractVecs <- vectorizeAbstracts(trainIn, trainOut)\n\n    freqMath <-\n      as.data.frame.table(table(abstractVecs$Math), stringsAsFactors = FALSE)\n    freqComp <-\n      as.data.frame.table(table(abstractVecs$Comp), stringsAsFactors = FALSE)\n    freqPhys <-\n      as.data.frame.table(table(abstractVecs$Phys), stringsAsFactors = FALSE)\n    freqStat <-\n      as.data.frame.table(table(abstractVecs$Stat), stringsAsFactors = FALSE)\n    freqMisc <-\n      as.data.frame.table(table(abstractVecs$Misc), stringsAsFactors = FALSE)\n    freqAll  <-\n      as.data.frame.table(table(unlist(abstractVecs)), stringsAsFactors = FALSE)\n\n    if (nrow(freqMisc) == 0) {\n      freqMisc <- freqAll\n      freqMisc[, 2] <- NA\n    }\n    tempList <-\n      list(freqAll, freqMath, freqComp, freqPhys, freqStat, freqMisc)\n    nameVec <- c(\"All\", \"Math\", \"Comp\", \"Phys\", \"Stat\", \"Misc\")\n    for (i in 1:length(tempList)) {\n      names(tempList[[i]]) <- c(\"Words\", nameVec[i])\n    }\n    fullFreq <-\n      Reduce(function(x, y)\n        merge(x, y, by = \"Words\", all = TRUE), tempList)\n    fullFreq[is.na(fullFreq)] <- 0\n    fullFreq[, 3:7] <- fullFreq[, 3:7] + alpha\n    fullFreq$All <- fullFreq$All + alpha * 4\n\n    wordCounts <- c(colSums(fullFreq[, 3:7]), sum(fullFreq[, 2]))\n\n    fullLike <- fullFreq[, c(1, 3:7)]\n    for (i in 2:ncol(fullLike)) {\n      fullLike[, i] <- fullLike[, i] / wordCounts[i - 1]\n    }\n\n    fullLike$Evidence <- fullFreq[, 2] / wordCounts[6]\n\n    if (length(priors) < 4 || !is.numeric(priors)) {\n      if (!is.null(priors)) {\n        warning(\n          \"You need at least 4 numeric values for your priors.\n          Using default method to generate priors.\",\n          call. = FALSE\n        )\n      }\n      priorMath <-\n        (wordCounts[1] / wordCounts[6]) * diffuse + (1 / 4) * (1 - diffuse)\n      priorComp <-\n        (wordCounts[2] / wordCounts[6]) * diffuse + (1 / 4) * (1 - diffuse)\n      priorPhys <-\n        (wordCounts[3] / wordCounts[6]) * diffuse + (1 / 4) * (1 - diffuse)\n      priorStat <-\n        (wordCounts[4] / wordCounts[6]) * diffuse + (1 / 4) * (1 - diffuse)\n      priorMisc <-\n        (wordCounts[5] / wordCounts[6]) * diffuse + (0) * (1 - diffuse)\n    } else{\n      priors <- abs(priors)\n      if (length(priors) > 5) {\n        priors <- priors[1:5]\n      } else{\n        priors <- c(priors, 0)\n      }\n      priors <- priors / sum(priors)\n\n      priorMath <- priors[1]\n      priorComp <- priors[2]\n      priorPhys <- priors[3]\n      priorStat <- priors[4]\n      priorMisc <- priors[5]\n    }\n\n    fullPrior <- c(priorMath, priorComp, priorPhys, priorStat, priorMisc)\n\n    ## Remove the stupid Misc column\n    fullLike <- fullLike[, -6]\n    fullPrior <- fullPrior[-5]\n\n    return(list(fullLike, fullPrior))\n    }\n",
    "created" : 1476382887541.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "351293191",
    "id" : "F698CBF5",
    "lastKnownWriteTime" : 1476496513,
    "last_content_update" : 1476496513298,
    "path" : "C:/Users/Renzo/Desktop/GradSchool/Comp551/Abstract Project/AbstractPackage/R/bayesProb.R",
    "project_path" : "R/bayesProb.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}