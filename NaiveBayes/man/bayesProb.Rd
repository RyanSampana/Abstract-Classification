% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bayesProb.R
\name{bayesProb}
\alias{bayesProb}
\title{Calculates likelihoods, priors, and evidence for classifying text data by
Category}
\usage{
bayesProb(trainIn, trainOut, alpha = 1, diffuse = 1, priors = NULL)
}
\arguments{
\item{trainIn}{A list of character vectors. (Output from cleanData())}

\item{trainOut}{A character vector denoting the Category to which each
element of the trainIn list belongs.}

\item{alpha}{The smoothing term for words whose frequencies are 0.}

\item{diffuse}{A number between 0 and 1 that scales the influence of
the prior, P(Category). (If priors are self-specified, this does nothing.)
P(Category') = P(Category)*(diffuse) + (1/4)*(1-diffuse).
This is a simple scaling system where, when diffuse=0, a uniform
prior is used (1/4 for each Category), when 1, the whole prior is used.}

\item{priors}{A vector of prior probabilities for each category. The order
matters and is as follows c(Math, Comp, Phys, Stat, Misc). If this is
left NULL, the default method to calculate priors is used.
(Including a prior for the Misc Category is optional.)}
}
\value{
A list with two entries. The first is a dataframe with six columns.
The first column identifies is a vector of words and the last column is
the evidence for each word. The middle five columns are the likelihoods
for each word for the Categories Math, Comp, Phys, Stat, and Misc, in that
order EDIT: (Misc is never used, so it's actually removed before output).
}
\description{
Calculates likelihoods, priors, and evidence for classifying text data by
Category for a particular word P(Category|word). The function determines:
P(word|Category) as the frequency of the word within each Category divided
by the total number of words within each Category; P(word) is the frequency
of the word among all Categories divided by the number of words among all
Categories; P(Category) is calculated as the total number of words within
each Category divided by the total number of words in all Categories.
(There is an option to adjust the influence of the prior.)
}
\examples{
#postProbs <- bayesProb(cleanTrainIn,cleanTrainOut,diffusePrior=0.1)
}

